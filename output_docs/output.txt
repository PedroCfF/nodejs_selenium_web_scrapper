Welcome to LangChain
On this page
Welcome to LangChain
LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:
Be data-aware: connect a language model to other sources of data
Be agentic: allow a language model to interact with its environment
The LangChain framework is designed with the above principles in mind.
Getting Started
Checkout the guide below for a walkthrough of how to get started using LangChain to create a Language Model application.
Quickstart, using LLMs
Quickstart, using Chat Models
Components
There are several main modules that LangChain provides support for. For each module we provide some examples to get started and get familiar with some of the concepts. Each example links to API documentation for the modules used.
These modules are, in increasing order of complexity:
Schema: This includes interfaces and base classes used throughout the library.
Models: This includes integrations with a variety of LLMs, Chat Models and Embeddings models.
Prompts: This includes prompt Templates and functionality to work with prompts like Output Parsers and Example Selectors
Indexes: This includes patterns and functionality for working with your own data, and making it ready to interact with language models (including document loaders, vectorstores, text splitters and retrievers).
Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.
Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.
Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.
API Reference
Here you can find the API reference for all of the modules in LangChain, as well as full documentation for all exported classes and functions.
Production
As you move from prototyping into production, we're developing resources to help you do so. These including:
Deployment: resources on how to deploy your application to production.
Events/Callbacks: resources on the events exposed by LangChain modules.
Tracing: resouces on how to use tracing to log and debug your application.
Additional Resources
Additional collection of resources we think may be useful as you develop your application!
LangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.
Discord: Join us on our Discord to discuss all things LangChain!
Production Support: As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.
Edit this pageGetting StartedSetup and Installation
On this page
Setup and Installation
INFO
Updating from <0.0.52? See this section for instructions.
Supported Environments
LangChain is written in TypeScript and can be used in:
Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x
Cloudflare Workers
Vercel / Next.js (Browser, Serverless and Edge functions)
Supabase Edge Functions
Browser
Deno
Quickstart
If you want to get started quickly on using LangChain in Node.js, clone this repository and follow the README instructions for a boilerplate project with those dependencies set up.
If you prefer to set things up yourself, or you want to run LangChain in other environments, read on for instructions.
Installation
To get started, install LangChain with the following command:
npm
Yarn
pnpm
npm install -S langchain


TypeScript
LangChain is written in TypeScript and provides type definitions for all of its public APIs.
Loading the library
ESM
LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
If you are using TypeScript in an ESM project we suggest updating your tsconfig.json to include the following:
tsconfig.json
{
  "compilerOptions": {
    ...
    "target": "ES2020", // or higher
    "module": "nodenext",
  }
}
CommonJS
LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:
const { OpenAI } = require("langchain/llms/openai");
Cloudflare Workers
LangChain can be used in Cloudflare Workers. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Vercel / Next.js
LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Deno / Supabase Edge Functions
LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:
import { OpenAI } from "https://esm.sh/langchain/llms/openai";
We recommend looking at our Supabase Template for an example of how to use LangChain in Supabase Edge Functions.
Browser
LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Updating from <0.0.52
If you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.
For example, if you were previously doing
import { OpenAI } from "langchain/llms";
you will now need to do
import { OpenAI } from "langchain/llms/openai";
This applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.
If you were using langchain/llms, see LLMs for updated import paths.
If you were using langchain/chat_models, see Chat Models for updated import paths.
If you were using langchain/embeddings, see Embeddings for updated import paths.
If you were using langchain/vectorstores, see Vector Stores for updated import paths.
If you were using langchain/document_loaders, see Document Loaders for updated import paths.
If you were using langchain/retrievers, see Retrievers for updated import paths.
Other modules are not affected by this change, and you can continue to import them from the same path.
Additionally, there are some breaking changes that were needed to support new environments:
import { Calculator } from "langchain/tools"; now moved to
import { Calculator } from "langchain/tools/calculator";
import { loadLLM } from "langchain/llms"; now moved to
import { loadLLM } from "langchain/llms/load";
import { loadAgent } from "langchain/agents"; now moved to
import { loadAgent } from "langchain/agents/load";
import { loadPrompt } from "langchain/prompts"; now moved to
import { loadPrompt } from "langchain/prompts/load";
import { loadChain } from "langchain/chains"; now moved to
import { loadChain } from "langchain/chains/load";
Unsupported: Node.js 16
We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.
You will have to make fetch available globally, either:
run your application with NODE_OPTIONS='--experimental-fetch' node ..., or
install node-fetch and follow the instructions here
Additionally you'll have to polyfill unstructuredClone, eg. by installing core-js and following the instructions here.
If you are running this on Node.js 18+, you do not need to do anything.
Edit this pageGetting StartedQuickstart, using LLMs
On this page
Quickstart, using LLMs
This tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.
Installation and Setup
To get started, follow the installation instructions to install LangChain.
Picking up a LLM
Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.
For this example, we will be using OpenAI's APIs, so no additional setup is required.
Building a Language Model Application
Now that we have installed LangChain, we can start building our language model application.
LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.
LLMs: Get Predictions from a Language Model
The most basic building block of LangChain is calling an LLM on some input. Let's walk through a simple example of how to do this. For this purpose, let's pretend we are building a service that generates a company name based on what the company makes.
In order to do this, we first need to import the LLM wrapper.
import { OpenAI } from "langchain/llms/openai";
We will then need to set the environment variable for the OpenAI key. Three options here:
We can do this by setting the value in a .env file and use the dotenv package to read it.
1.1. For OpenAI Api
OPENAI_API_KEY="..."
1.2. For Azure OpenAI:
AZURE_OPENAI_API_KEY="..."
AZURE_OPENAI_API_INSTANCE_NAME="..."
AZURE_OPENAI_API_DEPLOYMENT_NAME="..."
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME="..."
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME="..."
AZURE_OPENAI_API_VERSION="..."
Or we can export the environment variable with the following command in your shell:
2.1. For OpenAI Api
export OPENAI_API_KEY=sk-....
2.2. For Azure OpenAI:
export AZURE_OPENAI_API_KEY="..."
export AZURE_OPENAI_API_INSTANCE_NAME="..."
export AZURE_OPENAI_API_DEPLOYMENT_NAME="..."
export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME="..."
export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME="..."
export AZURE_OPENAI_API_VERSION="..."
Or we can do it when initializing the wrapper along with other arguments. In this example, we probably want the outputs to be MORE random, so we'll initialize it with a HIGH temperature.
3.1. For OpenAI Api
const model = new OpenAI({ openAIApiKey: "sk-...", temperature: 0.9 });
3.2. For Azure OpenAI:
const model = new OpenAI({
  azureOpenAIApiKey: "...",
  azureOpenAIApiInstanceName: "....",
  azureOpenAIApiDeploymentName: "....",
  azureOpenAIApiVersion: "....",
  temperature: 0.9
});
Once we have initialized the wrapper, we can now call it on some input!
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log(res);
{ res: '\n\nFantasy Sockery' }
Prompt Templates: Manage Prompts for LLMs
Calling an LLM is a great first step, but it's just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.
For example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.
This is easy to do with LangChain!
First lets define the prompt template:
import { PromptTemplate } from "langchain/prompts";

const template = "What is a good name for a company that makes {product}?";
const prompt = new PromptTemplate({
  template: template,
  inputVariables: ["product"],
});
Let's now see how this works! We can call the .format method to format it.
const res = await prompt.format({ product: "colorful socks" });
console.log(res);
{ res: 'What is a good name for a company that makes colorful socks?' }
Chains: Combine LLMs and Prompts in Multi-Step Workflows
Up until now, we've worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.
A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.
The most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.
Extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

const model = new OpenAI({ temperature: 0.9 });
const template = "What is a good name for a company that makes {product}?";
const prompt = new PromptTemplate({
  template: template,
  inputVariables: ["product"],
});
We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:
import { LLMChain } from "langchain/chains";

const chain = new LLMChain({ llm: model, prompt: prompt });
Now we can run that chain only specifying the product!
const res = await chain.call({ product: "colorful socks" });
console.log(res);
{ res: { text: '\n\nColorfulCo Sockery.' } }
There we go! There's the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.
Agents: Dynamically Run Chains Based on User Input
So far the chains we've looked at run in a predetermined order.
Agents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.
When used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.
In order to load agents, you should understand the following concepts:
Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.
LLM: The language model powering the agent.
Agent: The agent to use. This should be a string that references a support agent class. Because this tutorial focuses on the simplest, highest level API, this only covers using the standard supported agents.
For this example, you'll need to set the SerpAPI environment variables in the .env file.
SERPAPI_API_KEY="..."
Install serpapi package (Google Search API):
npm
Yarn
pnpm
npm install -S serpapi


Now we can get started!
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
console.log("Loaded agent.");

const input =
  "Who is Olivia Wilde's boyfriend?" +
  " What is his current age raised to the 0.23 power?";
console.log(`Executing with input "${input}"...`);

const result = await executor.call({ input });

console.log(`Got output ${result.output}`);
langchain-examples:start: Executing with input "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"...
langchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.
Memory: Add State to Chains and Agents
So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of "memory" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of "short-term memory". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of "long-term memory".
LangChain provides several specially created chains just for this purpose. This section walks through using one of those chains (the ConversationChain).
By default, the ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let's take a look at using this chain.
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log(res1);
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
const res2 = await chain.call({ input: "What's my name?" });
console.log(res2);
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
Streaming
You can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support tokenUsage reporting while streaming is enabled.
import { OpenAI } from "langchain/llms/openai";

// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const chat = new OpenAI({
  streaming: true,
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        process.stdout.write(token);
      },
    },
  ],
});

await chat.call("Write me a song about sparkling water.");
/*
Verse 1
Crystal clear and made with care
Sparkling water on my lips, so refreshing in the air
Fizzy bubbles, light and sweet
My favorite beverage I can‚Äôt help but repeat

Chorus
A toast to sparkling water, I‚Äôm feeling so alive
Let‚Äôs take a sip, and let‚Äôs take a drive
A toast to sparkling water, it‚Äôs the best I‚Äôve had in my life
It‚Äôs the best way to start off the night

Verse 2
It‚Äôs the perfect drink to quench my thirst
It‚Äôs the best way to stay hydrated, it‚Äôs the first
A few ice cubes, a splash of lime
It will make any day feel sublime
...
*/
API Reference:
OpenAI from langchain/llms/openai
Edit this pageGetting StartedQuickstart, using Chat Models
On this page
Quickstart, using Chat Models
Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different. Rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.
Chat model APIs are fairly new, so we are still figuring out the correct abstractions.
Installation and Setup
To get started, follow the installation instructions to install LangChain.
Getting Started
This section covers how to get started with chat models. The interface is based around messages rather than raw text.
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanChatMessage, SystemChatMessage } from "langchain/schema";

const chat = new ChatOpenAI({ temperature: 0 });
Here we create a chat model using the API key stored in the environment variable OPENAI_API_KEY or AZURE_OPENAI_API_KEY in case you are using Azure OpenAI. We'll be calling this chat model throughout this section.
‚ìò Note, if you are using Azure OpenAI make sure to also set the environment variables AZURE_OPENAI_API_INSTANCE_NAME, AZURE_OPENAI_API_DEPLOYMENT_NAME and AZURE_OPENAI_API_VERSION.
Chat Models: Message in, Message out
You can get chat completions by passing one or more messages to the chat model. The response will also be a message. The types of messages currently supported in LangChain are AIChatMessage, HumanChatMessage, SystemChatMessage, and a generic ChatMessage -- ChatMessage takes in an arbitrary role parameter, which we won't be using here. Most of the time, you'll just be dealing with HumanChatMessage, AIChatMessage, and SystemChatMessage.
const response = await chat.call([
  new HumanChatMessage(
    "Translate this sentence from English to French. I love programming."
  ),
]);

console.log(response);
AIChatMessage { text: "J'aime programmer." }
Multiple Messages
OpenAI's chat-based models (currently gpt-3.5-turbo and gpt-4 and in case of azure OpenAI gpt-4-32k) support multiple messages as input. See here for more information. Here is an example of sending a system and user message to the chat model:
‚ìò Note, if you are using Azure OpenAI make sure to change the deployment name to the deployment for the model you choose.
const responseB = await chat.call([
  new SystemChatMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanChatMessage("Translate: I love programming."),
]);

console.log(responseB);
AIChatMessage { text: "J'aime programmer." }
Multiple Completions
You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.
const responseC = await chat.generate([
  [
    new SystemChatMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanChatMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemChatMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanChatMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);

console.log(responseC);
{
  generations: [
    [
      {
        text: "J'aime programmer.",
        message: AIChatMessage { text: "J'aime programmer." },
      }
    ],
    [
      {
        text: "J'aime l'intelligence artificielle.",
        message: AIChatMessage { text: "J'aime l'intelligence artificielle." }
      }
    ]
  ]
}
Chat Prompt Templates: Manage Prompts for Chat Models
You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's formatPromptValue -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.
Continuing with the previous example:
import {
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
  ChatPromptTemplate,
} from "langchain/prompts";
First we create a reusable template:
const translationPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
Then we can use the template to generate a response:
const responseA = await chat.generatePrompt([
  await translationPrompt.formatPromptValue({
    input_language: "English",
    output_language: "French",
    text: "I love programming.",
  }),
]);

console.log(responseA);
{
  generations: [
    [
      {
        text: "J'aime programmer.",
        message: AIChatMessage { text: "J'aime programmer." }
      }
    ]
  ]
}
Model + Prompt = LLMChain
This pattern of asking for the completion of a formatted prompt is quite common, so we introduce the next piece of the puzzle: LLMChain
const chain = new LLMChain({
  prompt: translationPrompt,
  llm: chat,
});
Then you can call the chain:
const responseB = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});

console.log(responseB);
{ text: "J'aime programmer." }
Agents: Dynamically Run Chains Based on User Input
Finally, we introduce Tools and Agents, which extend the model with other abilities, such as search, or a calculator.
A tool is a function that takes a string (such as a search query) and returns a string (such as a search result). They also have a name and description, which are used by the chat model to identify which tool it should call.
class Tool {
  name: string;
  description: string;
  call(arg: string): Promise<string>;
}
An agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model.
interface AgentStep {
  action: AgentAction;
  observation: string;
}

interface AgentAction {
  tool: string; // Tool.name
  toolInput: string; // Tool.call argument
}

interface AgentFinish {
  returnValues: object;
}

class Agent {
  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;
}
To make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.
class AgentExecutor {
  // a simplified implementation
  run(inputs: object) {
    const steps = [];
    while (true) {
      const step = await this.agent.plan(steps, inputs);
      if (step instanceof AgentFinish) {
        return step.returnValues;
      }
      steps.push(step);
    }
  }
}
And finally, we can use the AgentExecutor to run an agent:
// Define the list of tools the agent can use
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
];
// Create the agent from the chat model and the tools
const agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);
// Create an executor, which calls to the agent until an answer is found
const executor = AgentExecutor.fromAgentAndTools({ agent, tools });

const responseG = await executor.run(
  "How many people live in canada as of 2023?"
);

console.log(responseG);
38,626,704.
Memory: Add State to Chains and Agents
You can also use the chain to store state. This is useful for eg. chatbots, where you want to keep track of the conversation history. MessagesPlaceholder is a special prompt template that will be replaced with the messages passed in each call.
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
  ),
  new MessagesPlaceholder("history"),
  HumanMessagePromptTemplate.fromTemplate("{input}"),
]);

const chain = new ConversationChain({
  memory: new BufferMemory({ returnMessages: true, memoryKey: "history" }),
  prompt: chatPrompt,
  llm: chat,
});
The chain will internally accumulate the messages sent to the model, and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times, and it remembers previous messages:
const responseH = await chain.call({
  input: "hi from London, how are you doing today",
});

console.log(responseH);
{
  response: "Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?"
}
const responseI = await chain.call({
  input: "Do you know where I am?",
});

console.log(responseI);
{
  response: "Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information."
}
Streaming
You can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support tokenUsage reporting while streaming is enabled.
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanChatMessage } from "langchain/schema";

const chat = new ChatOpenAI({
  streaming: true,
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        process.stdout.write(token);
      },
    },
  ],
});

await chat.call([
  new HumanChatMessage("Write me a song about sparkling water."),
]);
/*
Verse 1:
Bubbles rise, crisp and clear
Refreshing taste that brings us cheer
Sparkling water, so light and pure
Quenches our thirst, it's always secure

Chorus:
Sparkling water, oh how we love
Its fizzy bubbles and grace above
It's the perfect drink, anytime, anyplace
Refreshing as it gives us a taste

Verse 2:
From morning brunch to evening feast
It's the perfect drink for a treat
A sip of it brings a smile so bright
Our thirst is quenched in just one sip so light
...
*/
API Reference:
ChatOpenAI from langchain/chat_models/openai
HumanChatMessage from langchain/schema
Edit this pageComponentsSchema
Schema
This section speaks about interfaces that are used throughout the rest of the library.
üìÑÔ∏è Chat Messages
The primary interface through which end users interact with LLMs is a chat interface. For this reason, some model providers have started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user (or role). Right now the supported users are System, Human, and AI.
üìÑÔ∏è Document
Language models only know information about what they were trained on. In order to get them to answer questions or summarize other information you have to pass it to the language model. Therefore, it is very important to have a concept of a document.
üìÑÔ∏è Examples
Examples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.
Edit this pageComponentsModels
Models
INFO
Conceptual Guide
Models are a core component of LangChain. LangChain is not a provider of models, but rather provides a standard interface through which you can interact with a variety of language models. LangChain provides support for both text-based Large Language Models (LLMs), Chat Models, and Text Embedding models.
LLMs use a text-based input and output, while Chat Models use a message-based input and output.
Note: Chat model APIs are fairly new, so we are still figuring out the correct abstractions. If you have any feedback, please let us know!
All Models
üóÉÔ∏è Chat Models
2 items
üóÉÔ∏è Embeddings
2 items
üóÉÔ∏è LLMs
2 items
Advanced
This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section.
Both LLMs and Chat Models are built on top of the BaseLanguageModel class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.
The BaseLanguageModel class has two abstract methods: generatePrompt and getNumTokens, which are implemented by BaseChatModel and BaseLLM respectively.
BaseLLM is a subclass of BaseLanguageModel that provides a common interface for LLMs while BaseChatModel is a subclass of BaseLanguageModel that provides a common interface for chat models.
Edit this pageComponentsPrompts
Prompts
INFO
Conceptual Guide
LangChain provides several utilities to help manage prompts for language models, including chat models.
üóÉÔ∏è Prompt Templates
1 items
üìÑÔ∏è Output Parsers
Conceptual Guide
üìÑÔ∏è Example Selectors
Conceptual Guide
Edit this pageComponentsIndexes
Indexes
INFO
Conceptual Guide
This section deals with everything related to bringing your own data into LangChain, indexing it, and making it available for LLMs/Chat Models.
üóÉÔ∏è Document Loaders
1 items
üóÉÔ∏è Text Splitters
1 items
üóÉÔ∏è Vector Stores
1 items
üóÉÔ∏è Retrievers
11 items
Edit this pageComponentsMemory
On this page
Getting Started: Memory
INFO
Conceptual Guide
Memory is the concept of storing and retrieving data in the process of a conversation. There are two main methods, loadMemoryVariables and saveContext. The first method is used to retrieve data from memory (optionally using the current input values), and the second method is used to store data in memory.
export type InputValues = Record<string, any>;

export type OutputValues = Record<string, any>;

interface BaseMemory {
  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
NOTE
Do not share the same memory instance between two different chains, a memory instance represents the history of a single conversation
NOTE
If you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.
All Memory classes
üóÉÔ∏è Examples
9 items
Advanced
To implement your own memory class you have two options:
Subclassing BaseChatMemory
This is the easiest way to implement your own memory class. You can subclass BaseChatMemory, which takes care of saveContext by saving inputs and outputs as Chat Messages, and implement only the loadMemoryVariables method. This method is responsible for returning the memory variables that are relevant for the current input values.
abstract class BaseChatMemory extends BaseMemory {
  chatHistory: ChatMessageHistory;

  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;
}
Subclassing BaseMemory
If you want to implement a more custom memory class, you can subclass BaseMemory and implement both loadMemoryVariables and saveContext methods. The saveContext method is responsible for storing the input and output values in memory. The loadMemoryVariables method is responsible for returning the memory variables that are relevant for the current input values.
abstract class BaseMemory {
  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  abstract saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
Edit this pageComponentsChains
Getting Started: Chains
INFO
Conceptual Guide
Using a language model in isolation is fine for some applications, but it is often useful to combine language models with other sources of information, third-party APIs, or even other language models. This is where the concept of a chain comes in.
LangChain provides a standard interface for chains, as well as a number of built-in chains that can be used out of the box. You can also create your own chains.
üìÑÔ∏è LLM Chain
Conceptual Guide
üóÉÔ∏è Index Related Chains
3 items
üìÑÔ∏è Sequential Chain
Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario.
üóÉÔ∏è Other Chains
7 items
üìÑÔ∏è Prompt Selectors
Conceptual Guide
Advanced
To implement your own custom chain you can subclass BaseChain and implement the following methods:
import { CallbackManagerForChainRun } from "langchain/callbacks";
import { BaseChain as _ } from "langchain/chains";
import { BaseMemory } from "langchain/memory";
import { ChainValues } from "langchain/schema";

abstract class BaseChain {
  memory?: BaseMemory;

  /**
   * Run the core logic of this chain and return the output
   */
  abstract _call(
    values: ChainValues,
    runManager?: CallbackManagerForChainRun
  ): Promise<ChainValues>;

  /**
   * Return the string type key uniquely identifying this class of chain.
   */
  abstract _chainType(): string;

  /**
   * Return the list of input keys this chain expects to receive when called.
   */
  abstract get inputKeys(): string[];

  /**
   * Return the list of output keys this chain will produce when called.
   */
  abstract get outputKeys(): string[];
}
API Reference:
CallbackManagerForChainRun from langchain/callbacks
BaseChain from langchain/chains
BaseMemory from langchain/memory
ChainValues from langchain/schema
Subclassing BaseChain
The _call method is the main method custom chains must implement. It takes a record of inputs and returns a record of outputs. The inputs received should conform the inputKeys array, and the outputs returned should conform to the outputKeys array.
When implementing this method in a custom chain it's worth paying special attention to the runManager argument, which is what allows your custom chains to participate in the same callbacks system as the built-in chains.
If you call into another chain/model/agent inside your custom chain then you should pass it the result of calling runManager?.getChild() which will produce a new callback manager scoped to that inner run. An example:
import { BasePromptTemplate, PromptTemplate } from "langchain/prompts";
import { BaseLanguageModel } from "langchain/base_language";
import { CallbackManagerForChainRun } from "langchain/callbacks";
import { BaseChain, ChainInputs } from "langchain/chains";
import { ChainValues } from "langchain/schema";

export interface MyCustomChainInputs extends ChainInputs {
  llm: BaseLanguageModel;
  promptTemplate: string;
}

export class MyCustomChain extends BaseChain implements MyCustomChainInputs {
  llm: BaseLanguageModel;

  promptTemplate: string;

  prompt: BasePromptTemplate;

  constructor(fields: MyCustomChainInputs) {
    super(fields);
    this.llm = fields.llm;
    this.promptTemplate = fields.promptTemplate;
    this.prompt = PromptTemplate.fromTemplate(this.promptTemplate);
  }

  async _call(
    values: ChainValues,
    runManager?: CallbackManagerForChainRun
  ): Promise<ChainValues> {
    // Your custom chain logic goes here
    // This is just an example that mimics LLMChain
    const promptValue = await this.prompt.formatPromptValue(values);

    // Whenever you call a language model, or another chain, you should pass
    // a callback manager to it. This allows the inner run to be tracked by
    // any callbacks that are registered on the outer run.
    // You can always obtain a callback manager for this by calling
    // `runManager?.getChild()` as shown below.
    const result = await this.llm.generatePrompt(
      [promptValue],
      {},
      runManager?.getChild()
    );

    // If you want to log something about this run, you can do so by calling
    // methods on the runManager, as shown below. This will trigger any
    // callbacks that are registered for that event.
    runManager?.handleText("Log something about this run");

    return { output: result.generations[0][0].text };
  }

  _chainType(): string {
    return "my_custom_chain";
  }

  get inputKeys(): string[] {
    return ["input"];
  }

  get outputKeys(): string[] {
    return ["output"];
  }
}
API Reference:
BasePromptTemplate from langchain/prompts
PromptTemplate from langchain/prompts
BaseLanguageModel from langchain/base_language
CallbackManagerForChainRun from langchain/callbacks
BaseChain from langchain/chains
ChainInputs from langchain/chains
ChainValues from langchain/schema
Edit this pageComponentsAgents
Agents
INFO
Conceptual Guide
Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a ‚Äúagent‚Äù which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.
At the moment, there are two main types of agents:
Action Agents: these agents decide an action to take and take that action one step at a time
Plan-and-Execute Agents: these agents first decide a plan of actions to take, and then execute those actions one at a time.
When should you use each one?
Action Agents are more conventional, and good for small tasks.
For more complex or long running tasks, the initial planning step of Plan-and-Execute Agents helps to maintain long term objectives and focus, at the expense of generally more calls and higher latency.
These two agents are also not mutually exclusive - in fact, it is often best to have an Action Agent be in charge of the execution for the Plan and Execute agent.
Action Agents
The high-level pseudocode of an Action Agent looks something like:
Some user input is received
The agent decides which tool - if any - to use, and what the input to that tool should be
That tool is then called with that tool input, and an observation is recorded (this is just the output of calling that tool with that tool input).
That history of tool, tool input, and observation is passed back into the agent, and it decides what steps to take next
This is repeated until the agent decides it no longer needs to use a tool, and then it responds directly to the user.
interface AgentStep {
  action: AgentAction;
  observation: string;
}

interface AgentAction {
  tool: string; // Tool.name
  toolInput: string; // Tool.call argument
}

interface AgentFinish {
  returnValues: object;
}

class Agent {
  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;
}
Plan-and-Execute Agents
The high level pseudocode of a Plan-and-Execute Agent looks something like:
Some user input is received
The planner lists out the steps to take
The executor goes through the list of steps, executing them one-by-one until outputting the final result
The current implementation is to use an LLMChain as the planner and an Action Agent as the executor.
Go deeper
üóÉÔ∏è Agents
3 items
üóÉÔ∏è Agent Executors
1 items
üóÉÔ∏è Tools
6 items
üóÉÔ∏è Toolkits
4 items
üìÑÔ∏è Additional Functionality
We offer a number of additional features for Agents. You should also look at the LLM-specific features and Chat Model-specific features.
Edit this pageUse CasesPersonal Assistants
Personal Assistants
INFO
Conceptual Guide
We use "personal assistant" here in a very broad sense. Personal assistants have a few characteristics:
They can interact with the outside world
They have knowledge of your data
They remember your interactions
Really all of the functionality in LangChain is relevant for building a personal assistant. Highlighting specific parts:
Agent Documentation (for interacting with the outside world)
Index Documentation (for giving them knowledge of your data)
Memory (for helping them remember interactions)
Edit this pageUse CasesQuestion Answering
Question Answering
INFO
Conceptual Guide
Question answering in this context refers to question answering over your document data. There are a few different types of question answering:
Retrieval Question Answering: Use this to ingest documents, index them into a vectorstore, and then be able to ask questions about it.
Chat Retrieval: Similar to above in that you ingest and index documents, but this lets you have more a conversation (ask follow up questions, etc) rather than just asking one-off questions.
Indexing
For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).
Therefore, it is really important to understand how to create indexes, and so you should familiarize yourself with all the documentation related to that.
Indexes
Chains
After you create an index, you can then use it in a chain. You can just do normal question answering over it, or you can use it a conversational way. For an overview of these chains (and more) see the below documentation.
Index related chains
Agents
If you want to be able to answer more complex, multi-hop questions you should look into combining your indexes with an agent. For an example of how to do that, please see the below.
Vectorstore Agent
Edit this pageUse CasesTabular Question Answering
Tabular Question Answering
INFO
Conceptual Guide
Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables. This page covers all resources available in LangChain for working with data in this format.
Chains
If you are just getting started, and you have relatively small/simple tabular data, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.
SQL Database Chain
Agents
Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger databases and more complex schemas.
SQL Agent
Edit this pageUse CasesInteracting with APIs
Interacting with APIs
INFO
Conceptual Guide
Lots of data and information is stored behind APIs. This page covers all resources available in LangChain for working with APIs.
Chains
If you are just getting started and you have relatively simple APIs, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.
TODO: add an API chain and then add an example here.
Agents
Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger and more complex schemas.
OpenAPI Agent
Edit this pageUse CasesSummarization
Summarization
INFO
Conceptual Guide
A common use case is wanting to summarize long documents. This naturally runs into the context window limitations. Unlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything). So what do you do then?
To get started, we would recommend checking out the summarization chain which attacks this problem in a recursive manner.
Summarization Chain
Edit this pageUse CasesAutonomous Agents
Autonomous Agents
Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.
At the moment, Autonomous Agents are fairly experimental and based off of other open-source projects. By implementing these open source projects in LangChain primitives we can get the benefits of LangChain - easy switching and experimenting with multiple LLMs, usage of different vectorstores as memory, usage of LangChain's collection of tools.
üìÑÔ∏è AutoGPT
Original Repo//github.com/Significant-Gravitas/Auto-GPT
üìÑÔ∏è BabyAGI
Original Repo//github.com/yoheinakajima/babyagi
Edit this pageProductionEvents / Callbacks
On this page
Events / Callbacks
LangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.
You can subscribe to these events by using the callbacks argument available throughout the API. This method accepts a list of handler objects, which are expected to implement one or more of the methods described in the API docs.
Dive deeper
üìÑÔ∏è Creating callback handlers
Creating a custom handler
üìÑÔ∏è Callbacks in custom Chains
LangChain is designed to be extensible. You can add your own custom Chains and Agents to the library. This page will show you how to add callbacks to your custom Chains and Agents.
How to use callbacks
The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:
Constructor callbacks
Defined in the constructor, eg. new LLMChain({ callbacks: [handler] }), which will be used for all calls made on that object, and will be scoped to that object only, eg. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain.
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0,
  // This handler will be used for all calls made with this LLM.
  callbacks: [new ConsoleCallbackHandler()],
});
API Reference:
ConsoleCallbackHandler from langchain/callbacks
OpenAI from langchain/llms/openai
Request callbacks
Defined in the call()/run()/apply() methods used for issuing a request, eg. chain.call({ input: '...' }, [handler]), which will be used for that specific request only, and all sub-requests that it contains (eg. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0,
});

// This handler will be used only for this call.
const response = await llm.call("1 + 1 =", undefined, [
  new ConsoleCallbackHandler(),
]);
API Reference:
ConsoleCallbackHandler from langchain/callbacks
OpenAI from langchain/llms/openai
Verbose mode
The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. new LLMChain({ verbose: true }), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console. You can also enable verbose mode for the entire application by setting the environment variable LANGCHAIN_VERBOSE=true.
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

const chain = new LLMChain({
  llm: new OpenAI({ temperature: 0 }),
  prompt: PromptTemplate.fromTemplate("Hello, world!"),
  // This will enable logging of all Chain *and* LLM events to the console.
  verbose: true,
});
API Reference:
PromptTemplate from langchain/prompts
LLMChain from langchain/chains
OpenAI from langchain/llms/openai
When do you want to use each of these?
Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request, but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor.
Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method
Usage examples
Built-in handlers
LangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the ConsoleCallbackHandler, which simply logs all events to the console. In the future we will add more default handlers to the library. Note that when the verbose flag on the object is set to true, the ConsoleCallbackHandler will be invoked even without being explicitly passed in.
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const handler = new ConsoleCallbackHandler();
  const llm = new OpenAI({ temperature: 0, callbacks: [handler] });
  const prompt = PromptTemplate.fromTemplate("1 + {number} =");
  const chain = new LLMChain({ prompt, llm, callbacks: [handler] });

  const output = await chain.call({ number: 2 });
  /*
  Entering new llm_chain chain...
  Finished chain.
  */

  console.log(output);
  /*
  { text: ' 3\n\n3 - 1 = 2' }
   */

  // The non-enumerable key `__run` contains the runId.
  console.log(output.__run);
  /*
  { runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }
  */
};
API Reference:
ConsoleCallbackHandler from langchain/callbacks
LLMChain from langchain/chains
OpenAI from langchain/llms/openai
PromptTemplate from langchain/prompts
One-off handlers
You can create a one-off handler inline by passing a plain object to the callbacks argument. This object should implement the CallbackHandlerMethods interface. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.
import { OpenAI } from "langchain/llms/openai";

// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const chat = new OpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await chat.call("Tell me a joke.", undefined, [
  {
    handleLLMNewToken(token: string) {
      console.log({ token });
    },
  },
]);
console.log(response);
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }


Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
API Reference:
OpenAI from langchain/llms/openai
Multiple handlers
We offer a method on the CallbackManager class that allows you to create a one-off handler. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.
This is a more complete example that passes a CallbackManager to a ChatModel, and LLMChain, a Tool, and an Agent.
import { LLMChain } from "langchain/chains";
import { AgentExecutor, ZeroShotAgent } from "langchain/agents";
import { BaseCallbackHandler } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Calculator } from "langchain/tools/calculator";
import { AgentAction } from "langchain/schema";

export const run = async () => {
  // You can implement your own callback handler by extending BaseCallbackHandler
  class CustomHandler extends BaseCallbackHandler {
    name = "custom_handler";

    handleLLMNewToken(token: string) {
      console.log("token", { token });
    }

    handleLLMStart(llm: { name: string }, _prompts: string[]) {
      console.log("handleLLMStart", { llm });
    }

    handleChainStart(chain: { name: string }) {
      console.log("handleChainStart", { chain });
    }

    handleAgentAction(action: AgentAction) {
      console.log("handleAgentAction", action);
    }

    handleToolStart(tool: { name: string }) {
      console.log("handleToolStart", { tool });
    }
  }

  const handler1 = new CustomHandler();

  // Additionally, you can use the `fromMethods` method to create a callback handler
  const handler2 = BaseCallbackHandler.fromMethods({
    handleLLMStart(llm, _prompts: string[]) {
      console.log("handleLLMStart: I'm the second handler!!", { llm });
    },
    handleChainStart(chain) {
      console.log("handleChainStart: I'm the second handler!!", { chain });
    },
    handleAgentAction(action) {
      console.log("handleAgentAction", action);
    },
    handleToolStart(tool) {
      console.log("handleToolStart", { tool });
    },
  });

  // You can restrict callbacks to a particular object by passing it upon creation
  const model = new ChatOpenAI({
    temperature: 0,
    callbacks: [handler2], // this will issue handler2 callbacks related to this model
    streaming: true, // needed to enable streaming, which enables handleLLMNewToken
  });

  const tools = [new Calculator()];
  const agentPrompt = ZeroShotAgent.createPrompt(tools);

  const llmChain = new LLMChain({
    llm: model,
    prompt: agentPrompt,
    callbacks: [handler2], // this will issue handler2 callbacks related to this chain
  });
  const agent = new ZeroShotAgent({
    llmChain,
    allowedTools: ["search"],
  });

  const agentExecutor = AgentExecutor.fromAgentAndTools({
    agent,
    tools,
  });

  /*
   * When we pass the callback handler to the agent executor, it will be used for all
   * callbacks related to the agent and all the objects involved in the agent's
   * execution, in this case, the Tool, LLMChain, and LLM.
   *
   * The `handler2` callback handler will only be used for callbacks related to the
   * LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.
   */
  const result = await agentExecutor.call(
    {
      input: "What is 2 to the power of 8",
    },
    [handler1]
  ); // this is needed to see handleAgentAction
  /*
  handleChainStart { chain: { name: 'agent_executor' } }
  handleChainStart { chain: { name: 'llm_chain' } }
  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
  handleLLMStart { llm: { name: 'openai' } }
  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
  token { token: '' }
  token { token: 'I' }
  token { token: ' can' }
  token { token: ' use' }
  token { token: ' the' }
  token { token: ' calculator' }
  token { token: ' tool' }
  token { token: ' to' }
  token { token: ' solve' }
  token { token: ' this' }
  token { token: '.\n' }
  token { token: 'Action' }
  token { token: ':' }
  token { token: ' calculator' }
  token { token: '\n' }
  token { token: 'Action' }
  token { token: ' Input' }
  token { token: ':' }
  token { token: ' ' }
  token { token: '2' }
  token { token: '^' }
  token { token: '8' }
  token { token: '' }
  handleAgentAction {
    tool: 'calculator',
    toolInput: '2^8',
    log: 'I can use the calculator tool to solve this.\n' +
      'Action: calculator\n' +
      'Action Input: 2^8'
  }
  handleToolStart { tool: { name: 'calculator' } }
  handleChainStart { chain: { name: 'llm_chain' } }
  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
  handleLLMStart { llm: { name: 'openai' } }
  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
  token { token: '' }
  token { token: 'That' }
  token { token: ' was' }
  token { token: ' easy' }
  token { token: '!\n' }
  token { token: 'Final' }
  token { token: ' Answer' }
  token { token: ':' }
  token { token: ' ' }
  token { token: '256' }
  token { token: '' }
  */

  console.log(result);
  /*
  {
    output: '256',
    __run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }
  }
  */
};
API Reference:
LLMChain from langchain/chains
AgentExecutor from langchain/agents
ZeroShotAgent from langchain/agents
BaseCallbackHandler from langchain/callbacks
ChatOpenAI from langchain/chat_models/openai
Calculator from langchain/tools/calculator
AgentAction from langchain/schema
Edit this pageProductionDeployment
On this page
Deployment
You've built your LangChain app and now you're looking to deploy it to production? You've come to the right place. This guide will walk you through the options you have for deploying your app, and the considerations you should make when doing so.
Overview
LangChain is a library for building applications that use language models. It is not a web framework, and does not provide any built-in functionality for serving your app over the web. Instead, it provides a set of tools that you can integrate in your API or backend server.
There are a couple of high-level options for deploying your app:
Deploying to a VM or container
Persistent filesystem means you can save and load files from disk
Always-running process means you can cache some things in memory
You can support long-running requests, such as WebSockets
Deploying to a serverless environment
No persistent filesystem means you can load files from disk, but not save them for later
Cold start means you can't cache things in memory and expect them to be cached between requests
Function timeouts mean you can't support long-running requests, such as WebSockets
Some other considerations include:
Do you deploy your backend and frontend together, or separately?
Do you deploy your backend co-located with your database, or separately?
As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.
Deployment Options
See below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.
Deploying to Fly.io
Fly.io is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.
See our Fly.io template for an example of how to deploy your app to Fly.io.
Deploying to Kinsta
Kinsta is a developer-centric cloud host platform.
Use our hello-world template for an example of how to deploy your next LangChain app at Kinsta in minutes.
Edit this pageProductionTracing
On this page
Tracing
Similar to the Python langchain package, JS langchain also supports tracing.
You can view an overview of tracing here. To spin up the tracing backend, run docker compose up (or docker-compose up if on using an older version of docker) in the langchain directory. You can also use the langchain-server command if you have the python langchain package installed.
Here's an example of how to use tracing in langchain.js. All that needs to be done is setting the LANGCHAIN_TRACING environment variable to true.
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";
import process from "process";

export const run = async () => {
  process.env.LANGCHAIN_TRACING = "true";
  const model = new OpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "zero-shot-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
Concurrency
Tracing works with concurrency out of the box.
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";
import process from "process";

export const run = async () => {
  process.env.LANGCHAIN_TRACING = "true";
  const model = new OpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "zero-shot-react-description",
    verbose: true,
  });

  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.
  const [resultA, resultB, resultC] = await Promise.all([
    executor.call({ input }),
    executor.call({ input }),
    executor.call({ input }),
  ]);

  console.log(`Got output ${resultA.output} ${resultA.__run.runId}`);
  console.log(`Got output ${resultB.output} ${resultB.__run.runId}`);
  console.log(`Got output ${resultC.output} ${resultC.__run.runId}`);

  /*
    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. b8fb98aa-07a5-45bd-b593-e8d7376b05ca
    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. c8d916d5-ca1d-4702-8dd7-cab5e438578b
    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. bf5fe04f-ef29-4e55-8ce1-e4aa974f9484
    */
};
Edit this pageGetting StartedSetup and Installation
On this page
Setup and Installation
INFO
Updating from <0.0.52? See this section for instructions.
Supported Environments
LangChain is written in TypeScript and can be used in:
Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x
Cloudflare Workers
Vercel / Next.js (Browser, Serverless and Edge functions)
Supabase Edge Functions
Browser
Deno
Quickstart
If you want to get started quickly on using LangChain in Node.js, clone this repository and follow the README instructions for a boilerplate project with those dependencies set up.
If you prefer to set things up yourself, or you want to run LangChain in other environments, read on for instructions.
Installation
To get started, install LangChain with the following command:
npm
Yarn
pnpm
npm install -S langchain


TypeScript
LangChain is written in TypeScript and provides type definitions for all of its public APIs.
Loading the library
ESM
LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
If you are using TypeScript in an ESM project we suggest updating your tsconfig.json to include the following:
tsconfig.json
{
  "compilerOptions": {
    ...
    "target": "ES2020", // or higher
    "module": "nodenext",
  }
}
CommonJS
LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:
const { OpenAI } = require("langchain/llms/openai");
Cloudflare Workers
LangChain can be used in Cloudflare Workers. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Vercel / Next.js
LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Deno / Supabase Edge Functions
LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:
import { OpenAI } from "https://esm.sh/langchain/llms/openai";
We recommend looking at our Supabase Template for an example of how to use LangChain in Supabase Edge Functions.
Browser
LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:
import { OpenAI } from "langchain/llms/openai";
Updating from <0.0.52
If you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.
For example, if you were previously doing
import { OpenAI } from "langchain/llms";
you will now need to do
import { OpenAI } from "langchain/llms/openai";
This applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.
If you were using langchain/llms, see LLMs for updated import paths.
If you were using langchain/chat_models, see Chat Models for updated import paths.
If you were using langchain/embeddings, see Embeddings for updated import paths.
If you were using langchain/vectorstores, see Vector Stores for updated import paths.
If you were using langchain/document_loaders, see Document Loaders for updated import paths.
If you were using langchain/retrievers, see Retrievers for updated import paths.
Other modules are not affected by this change, and you can continue to import them from the same path.
Additionally, there are some breaking changes that were needed to support new environments:
import { Calculator } from "langchain/tools"; now moved to
import { Calculator } from "langchain/tools/calculator";
import { loadLLM } from "langchain/llms"; now moved to
import { loadLLM } from "langchain/llms/load";
import { loadAgent } from "langchain/agents"; now moved to
import { loadAgent } from "langchain/agents/load";
import { loadPrompt } from "langchain/prompts"; now moved to
import { loadPrompt } from "langchain/prompts/load";
import { loadChain } from "langchain/chains"; now moved to
import { loadChain } from "langchain/chains/load";
Unsupported: Node.js 16
We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.
You will have to make fetch available globally, either:
run your application with NODE_OPTIONS='--experimental-fetch' node ..., or
install node-fetch and follow the instructions here
Additionally you'll have to polyfill unstructuredClone, eg. by installing core-js and following the instructions here.
If you are running this on Node.js 18+, you do not need to do anything.
Edit this pageAPI Reference
On this page
langchain
Index
Modules
agents
agents/load
base_language
cache
cache/redis
callbacks
chains
chains/load
chains/query_constructor
chains/query_constructor/ir
chat_models/anthropic
chat_models/base
chat_models/openai
client
docstore
document
document_loaders/base
document_loaders/fs/buffer
document_loaders/fs/csv
document_loaders/fs/directory
document_loaders/fs/docx
document_loaders/fs/epub
document_loaders/fs/json
document_loaders/fs/notion
document_loaders/fs/pdf
document_loaders/fs/srt
document_loaders/fs/text
document_loaders/fs/unstructured
document_loaders/web/apify_dataset
document_loaders/web/cheerio
document_loaders/web/college_confidential
document_loaders/web/confluence
document_loaders/web/gitbook
document_loaders/web/github
document_loaders/web/hn
document_loaders/web/imsdb
document_loaders/web/playwright
document_loaders/web/puppeteer
document_loaders/web/s3
embeddings/base
embeddings/cohere
embeddings/fake
embeddings/hf
embeddings/openai
embeddings/tensorflow
experimental/autogpt
experimental/babyagi
experimental/plan_and_execute
llms/base
llms/cohere
llms/hf
llms/load
llms/openai
llms/replicate
llms/sagemaker_endpoint
memory
output_parsers
output_parsers/expression
prompts
prompts/load
retrievers/contextual_compression
retrievers/databerry
retrievers/document_compressors
retrievers/document_compressors/chain_extract
retrievers/hyde
retrievers/metal
retrievers/remote
retrievers/self_query
retrievers/supabase
retrievers/time_weighted
schema
schema/output_parser
schema/query_constructor
sql_db
stores/file/in_memory
stores/file/node
stores/message/dynamodb
stores/message/redis
text_splitter
tools
tools/aws_lambda
tools/calculator
tools/webbrowser
vectorstores/base
vectorstores/chroma
vectorstores/faiss
vectorstores/hnswlib
vectorstores/memory
vectorstores/milvus
vectorstores/mongo
vectorstores/myscale
vectorstores/opensearch
vectorstores/pinecone
vectorstores/prisma
vectorstores/redis
vectorstores/supabase
vectorstores/weaviate
Edit this page